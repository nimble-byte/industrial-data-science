{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Since this is a handin exercise I will shortly outline the setup required for this notebook to run (assuming I can only hand in the `.ipynb` file). The notebook pulls from the dependencies in the first code block. To install all relevant libraries run (assuming you have `jupyter` installed):\n",
    "\n",
    "```shell\n",
    "python -m pip install pandas tensorflow sklearn\n",
    "```\n",
    "\n",
    "Additionally it required the data and labels to be available as follows:\n",
    "1. the batch data is located in a `data` folder next to the notebook\n",
    "2. the label information is stored as `labels.csv` next to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "First the data needs to be loaded (first codeblock) and then go through some minor transformations before building the training, test and validation datasets. For this purpose all data is loaded into a flat dataframe that includes the batch and corresponding label in addition to the actual features `zeit`, `sensorid` und `messwert`. In preparation for the model training the `zeit` is transformed to float values and the data types of `sensorid` and `label` are adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the labels.csv\n",
    "labels = pd.read_csv('labels.csv', index_col=0)\n",
    "labels = labels.sort_values('id')\n",
    "\n",
    "# grab filenames from the data directory\n",
    "filenames = os.listdir('data')\n",
    "filenames.sort()\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# parse and concatenate all csv files into df\n",
    "for filename in filenames:\n",
    "  if filename.endswith('.csv'):\n",
    "    batch = pd.read_csv(os.path.join('data',filename), index_col=0)\n",
    "    batch['batch'] = int(filename.replace('.csv', ''))\n",
    "    dataframes.append(batch)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# clean up original dataframes\n",
    "del dataframes\n",
    "\n",
    "# add label column (if it is not already available)\n",
    "if (not 'label' in df.columns):\n",
    "  df = df.merge(labels, left_on=[\"batch\"], right_on=[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_float(inputstr):\n",
    "  hours, minutes, seconds = map(float, inputstr.split(':'))\n",
    "\n",
    "  # return hours * 3600 + minutes * 60 + seconds\n",
    "  # this is sufficient because hours should always be 0\n",
    "  return minutes * 60 + seconds\n",
    "\n",
    "if (not df['sensorid'].dtype == 'int'):\n",
    "  df['sensorid'] = df['sensorid'].astype('int')\n",
    "if (not df['label'].dtype == 'category'):\n",
    "  df['label'] = df['label'].astype('category')\n",
    "if (not df['zeit'].dtype == 'float64'):\n",
    "  df['zeit'] = df['zeit'].apply(time_to_float)\n",
    "\n",
    "# print(df[:10])\n",
    "# print(labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Data\n",
    "\n",
    "This step was only introduced once I decided to use a recurrent neural network (specifically an LSTM). The flat dataframe created before has to be broken down into small sequences that will be fed to the model. As to not created sequences with mixed labels, the data was grouped by `batch`. Additionally it is important that each batch is sorted by `zeit` since order of readings in sequences is relevant. After splitting each batch into equally sized sequences (dropping any additional readings at the end that were not able to make a full sequence), the data is split into training, test and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 256\n",
    "\n",
    "sequences = []\n",
    "sequence_labels = []\n",
    "\n",
    "# build sequences based on chosen sequence length\n",
    "for batch, readings in df.groupby('batch'):\n",
    "  readings = readings.sort_values('zeit')\n",
    "  for i in range(0, len(readings) - SEQUENCE_LENGTH, SEQUENCE_LENGTH):\n",
    "    sequence = readings.iloc[i:i + SEQUENCE_LENGTH]\n",
    "    sequences.append(sequence[['zeit', 'sensorid', 'messwert']].values)\n",
    "    sequence_labels.append(sequence['label'].values[0])\n",
    "\n",
    "# transform to numpy arrays for tensorflow\n",
    "sequences = np.array(sequences)\n",
    "sequence_labels = np.array(sequence_labels)\n",
    "\n",
    "# split into train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, sequence_labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling & Training\n",
    "\n",
    "The initial plan was to train a simple NN, that is fed `zeit`, `sensorid` and `messwert` as features to predict the label, but it became apparent quickly, that this approach would not lead to the reuqired accurracy. After some additional research, I settled on an LSTM which is able to detect and learn patterns in the sequences. Batch size and the number of LSTM units was determined using a grid search and running it over night to determine the best `SEQUENCE_LENGTH`, `LSTM_UNITS` and `BATCH_SIZE`. The grid search revealed, that among the chosen options the largest options for `SEQUENCE_LENGTH` and `LSTM_UNITS` performed the best with an accuracy of about 76%.\n",
    "\n",
    "The model itself is implemented as Sequential Model using Tensorflow keras. It consists of 2 layers:\n",
    "\n",
    "- the aforementioned LSTM layer, handling the input and pattern detection\n",
    "- a densely connected layer using softmax activations to produce the predictions\n",
    "\n",
    "The model was then trained using the adam optimiser and set up for early training termination, based on the validation loss each round to prevent overfitting. Each 8 epochs the model weights were saved, as a safety measure should model training crash at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "153/153 [==============================] - 38s 244ms/step - loss: 1.1111 - accuracy: 0.3352 - val_loss: 1.1106 - val_accuracy: 0.3482\n",
      "Epoch 2/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 1.1059 - accuracy: 0.3489 - val_loss: 1.1064 - val_accuracy: 0.3523\n",
      "Epoch 3/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 1.0988 - accuracy: 0.3594 - val_loss: 1.0987 - val_accuracy: 0.3564\n",
      "Epoch 4/256\n",
      "153/153 [==============================] - 37s 240ms/step - loss: 1.0972 - accuracy: 0.3663 - val_loss: 1.1017 - val_accuracy: 0.3494\n",
      "Epoch 5/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 1.0929 - accuracy: 0.3746 - val_loss: 1.1027 - val_accuracy: 0.3732\n",
      "Epoch 6/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 1.0904 - accuracy: 0.3791 - val_loss: 1.0920 - val_accuracy: 0.3638\n",
      "Epoch 7/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 1.0901 - accuracy: 0.3748 - val_loss: 1.0911 - val_accuracy: 0.3658\n",
      "Epoch 8/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 1.0863 - accuracy: 0.3831 - val_loss: 1.0929 - val_accuracy: 0.3720\n",
      "Epoch 9/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 1.0913 - accuracy: 0.3734 - val_loss: 1.1018 - val_accuracy: 0.3421\n",
      "Epoch 10/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 1.0873 - accuracy: 0.3778 - val_loss: 1.0959 - val_accuracy: 0.3691\n",
      "Epoch 11/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 1.0880 - accuracy: 0.3777 - val_loss: 1.0925 - val_accuracy: 0.3761\n",
      "Epoch 12/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 1.0855 - accuracy: 0.3787 - val_loss: 1.0982 - val_accuracy: 0.3327\n",
      "Epoch 13/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 1.0849 - accuracy: 0.3921 - val_loss: 1.0901 - val_accuracy: 0.3679\n",
      "Epoch 14/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 1.0843 - accuracy: 0.3906 - val_loss: 1.0947 - val_accuracy: 0.3429\n",
      "Epoch 15/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 1.0863 - accuracy: 0.3883 - val_loss: 1.0968 - val_accuracy: 0.3781\n",
      "Epoch 16/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 1.0801 - accuracy: 0.3984 - val_loss: 1.0900 - val_accuracy: 0.3740\n",
      "Epoch 17/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 1.0826 - accuracy: 0.3864 - val_loss: 1.0913 - val_accuracy: 0.3712\n",
      "Epoch 18/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 1.0806 - accuracy: 0.3999 - val_loss: 1.0943 - val_accuracy: 0.3310\n",
      "Epoch 19/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 1.0789 - accuracy: 0.4005 - val_loss: 1.0875 - val_accuracy: 0.3953\n",
      "Epoch 20/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 1.0784 - accuracy: 0.4003 - val_loss: 1.0953 - val_accuracy: 0.3728\n",
      "Epoch 21/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 1.0770 - accuracy: 0.4086 - val_loss: 1.0900 - val_accuracy: 0.3740\n",
      "Epoch 22/256\n",
      "153/153 [==============================] - 35s 232ms/step - loss: 1.0771 - accuracy: 0.4021 - val_loss: 1.0895 - val_accuracy: 0.3945\n",
      "Epoch 23/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 1.0761 - accuracy: 0.4069 - val_loss: 1.0936 - val_accuracy: 0.3871\n",
      "Epoch 24/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 1.0752 - accuracy: 0.4043 - val_loss: 1.0897 - val_accuracy: 0.3761\n",
      "Epoch 25/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 1.0724 - accuracy: 0.4105 - val_loss: 1.0780 - val_accuracy: 0.4097\n",
      "Epoch 26/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 1.0693 - accuracy: 0.4135 - val_loss: 1.0857 - val_accuracy: 0.4109\n",
      "Epoch 27/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 1.0637 - accuracy: 0.4270 - val_loss: 1.0678 - val_accuracy: 0.4170\n",
      "Epoch 28/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 1.0566 - accuracy: 0.4326 - val_loss: 1.0583 - val_accuracy: 0.4248\n",
      "Epoch 29/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 1.0416 - accuracy: 0.4590 - val_loss: 1.0425 - val_accuracy: 0.4396\n",
      "Epoch 30/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 1.0182 - accuracy: 0.4799 - val_loss: 1.0994 - val_accuracy: 0.4007\n",
      "Epoch 31/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.9756 - accuracy: 0.5120 - val_loss: 0.9400 - val_accuracy: 0.5133\n",
      "Epoch 32/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.8823 - accuracy: 0.5693 - val_loss: 0.7698 - val_accuracy: 0.6071\n",
      "Epoch 33/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.8066 - accuracy: 0.5973 - val_loss: 0.8073 - val_accuracy: 0.5891\n",
      "Epoch 34/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.7700 - accuracy: 0.6140 - val_loss: 0.7376 - val_accuracy: 0.6141\n",
      "Epoch 35/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.7135 - accuracy: 0.6385 - val_loss: 0.6792 - val_accuracy: 0.6510\n",
      "Epoch 36/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.6800 - accuracy: 0.6584 - val_loss: 0.6484 - val_accuracy: 0.6825\n",
      "Epoch 37/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.6694 - accuracy: 0.6687 - val_loss: 0.6322 - val_accuracy: 0.6592\n",
      "Epoch 38/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.6852 - accuracy: 0.6546 - val_loss: 0.6970 - val_accuracy: 0.6784\n",
      "Epoch 39/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.6557 - accuracy: 0.6703 - val_loss: 0.6211 - val_accuracy: 0.6968\n",
      "Epoch 40/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.6492 - accuracy: 0.6725 - val_loss: 0.5935 - val_accuracy: 0.6964\n",
      "Epoch 41/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.6168 - accuracy: 0.6885 - val_loss: 0.6305 - val_accuracy: 0.6706\n",
      "Epoch 42/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.6646 - accuracy: 0.6723 - val_loss: 0.5904 - val_accuracy: 0.7235\n",
      "Epoch 43/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.6107 - accuracy: 0.6998 - val_loss: 0.5777 - val_accuracy: 0.7075\n",
      "Epoch 44/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.6012 - accuracy: 0.7089 - val_loss: 0.5727 - val_accuracy: 0.7095\n",
      "Epoch 45/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.6165 - accuracy: 0.6937 - val_loss: 0.6432 - val_accuracy: 0.6673\n",
      "Epoch 46/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5849 - accuracy: 0.7215 - val_loss: 0.6507 - val_accuracy: 0.6673\n",
      "Epoch 47/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.6017 - accuracy: 0.7058 - val_loss: 0.6749 - val_accuracy: 0.6518\n",
      "Epoch 48/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5770 - accuracy: 0.7177 - val_loss: 0.5551 - val_accuracy: 0.7304\n",
      "Epoch 49/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5628 - accuracy: 0.7339 - val_loss: 0.6576 - val_accuracy: 0.6469\n",
      "Epoch 50/256\n",
      "153/153 [==============================] - 35s 232ms/step - loss: 0.6196 - accuracy: 0.6953 - val_loss: 0.5574 - val_accuracy: 0.7247\n",
      "Epoch 51/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5731 - accuracy: 0.7174 - val_loss: 0.5711 - val_accuracy: 0.7095\n",
      "Epoch 52/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5743 - accuracy: 0.7177 - val_loss: 0.5791 - val_accuracy: 0.7112\n",
      "Epoch 53/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.5609 - accuracy: 0.7307 - val_loss: 0.5464 - val_accuracy: 0.7435\n",
      "Epoch 54/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5603 - accuracy: 0.7291 - val_loss: 0.6533 - val_accuracy: 0.6583\n",
      "Epoch 55/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.5553 - accuracy: 0.7342 - val_loss: 0.5492 - val_accuracy: 0.7251\n",
      "Epoch 56/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5743 - accuracy: 0.7175 - val_loss: 0.5835 - val_accuracy: 0.7251\n",
      "Epoch 57/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.5509 - accuracy: 0.7332 - val_loss: 0.5380 - val_accuracy: 0.7403\n",
      "Epoch 58/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.5616 - accuracy: 0.7306 - val_loss: 0.7576 - val_accuracy: 0.6657\n",
      "Epoch 59/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5445 - accuracy: 0.7408 - val_loss: 0.5288 - val_accuracy: 0.7522\n",
      "Epoch 60/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5546 - accuracy: 0.7371 - val_loss: 0.5319 - val_accuracy: 0.7550\n",
      "Epoch 61/256\n",
      "153/153 [==============================] - 35s 232ms/step - loss: 0.5451 - accuracy: 0.7419 - val_loss: 0.6324 - val_accuracy: 0.6895\n",
      "Epoch 62/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5514 - accuracy: 0.7400 - val_loss: 0.5494 - val_accuracy: 0.7517\n",
      "Epoch 63/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5331 - accuracy: 0.7531 - val_loss: 0.6674 - val_accuracy: 0.6624\n",
      "Epoch 64/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 0.5336 - accuracy: 0.7499 - val_loss: 0.5657 - val_accuracy: 0.7345\n",
      "Epoch 65/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5252 - accuracy: 0.7526 - val_loss: 0.5170 - val_accuracy: 0.7546\n",
      "Epoch 66/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5346 - accuracy: 0.7417 - val_loss: 0.5899 - val_accuracy: 0.7227\n",
      "Epoch 67/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.5504 - accuracy: 0.7345 - val_loss: 0.5297 - val_accuracy: 0.7513\n",
      "Epoch 68/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.5300 - accuracy: 0.7494 - val_loss: 0.6307 - val_accuracy: 0.6952\n",
      "Epoch 69/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 0.5326 - accuracy: 0.7482 - val_loss: 0.5807 - val_accuracy: 0.7165\n",
      "Epoch 70/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.5282 - accuracy: 0.7504 - val_loss: 0.5128 - val_accuracy: 0.7644\n",
      "Epoch 71/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.5290 - accuracy: 0.7502 - val_loss: 0.5221 - val_accuracy: 0.7550\n",
      "Epoch 72/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.5302 - accuracy: 0.7456 - val_loss: 0.5828 - val_accuracy: 0.7218\n",
      "Epoch 73/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5251 - accuracy: 0.7511 - val_loss: 0.5676 - val_accuracy: 0.7358\n",
      "Epoch 74/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5204 - accuracy: 0.7528 - val_loss: 0.5228 - val_accuracy: 0.7485\n",
      "Epoch 75/256\n",
      "153/153 [==============================] - 35s 232ms/step - loss: 0.5206 - accuracy: 0.7563 - val_loss: 0.6085 - val_accuracy: 0.7132\n",
      "Epoch 76/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5172 - accuracy: 0.7540 - val_loss: 0.5584 - val_accuracy: 0.7317\n",
      "Epoch 77/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5140 - accuracy: 0.7596 - val_loss: 0.5226 - val_accuracy: 0.7583\n",
      "Epoch 78/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.5034 - accuracy: 0.7645 - val_loss: 0.5643 - val_accuracy: 0.7349\n",
      "Epoch 79/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5142 - accuracy: 0.7605 - val_loss: 0.5288 - val_accuracy: 0.7489\n",
      "Epoch 80/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4931 - accuracy: 0.7722 - val_loss: 0.5145 - val_accuracy: 0.7579\n",
      "Epoch 81/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.5090 - accuracy: 0.7616 - val_loss: 0.5158 - val_accuracy: 0.7575\n",
      "Epoch 82/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.5124 - accuracy: 0.7597 - val_loss: 0.5385 - val_accuracy: 0.7489\n",
      "Epoch 83/256\n",
      "153/153 [==============================] - 35s 232ms/step - loss: 0.5467 - accuracy: 0.7381 - val_loss: 0.6911 - val_accuracy: 0.6551\n",
      "Epoch 84/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5000 - accuracy: 0.7658 - val_loss: 0.5031 - val_accuracy: 0.7644\n",
      "Epoch 85/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.5399 - accuracy: 0.7406 - val_loss: 0.5768 - val_accuracy: 0.7181\n",
      "Epoch 86/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.4863 - accuracy: 0.7742 - val_loss: 0.5486 - val_accuracy: 0.7415\n",
      "Epoch 87/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4920 - accuracy: 0.7715 - val_loss: 0.5299 - val_accuracy: 0.7456\n",
      "Epoch 88/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.5171 - accuracy: 0.7580 - val_loss: 0.6486 - val_accuracy: 0.6919\n",
      "Epoch 89/256\n",
      "153/153 [==============================] - 35s 232ms/step - loss: 0.5241 - accuracy: 0.7514 - val_loss: 0.6426 - val_accuracy: 0.7067\n",
      "Epoch 90/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4942 - accuracy: 0.7695 - val_loss: 0.5205 - val_accuracy: 0.7542\n",
      "Epoch 91/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.5167 - accuracy: 0.7565 - val_loss: 0.5140 - val_accuracy: 0.7522\n",
      "Epoch 92/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.4932 - accuracy: 0.7656 - val_loss: 0.5230 - val_accuracy: 0.7522\n",
      "Epoch 93/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.5228 - accuracy: 0.7511 - val_loss: 0.5837 - val_accuracy: 0.7259\n",
      "Epoch 94/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5183 - accuracy: 0.7547 - val_loss: 0.5296 - val_accuracy: 0.7472\n",
      "Epoch 95/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4881 - accuracy: 0.7729 - val_loss: 0.5130 - val_accuracy: 0.7599\n",
      "Epoch 96/256\n",
      "153/153 [==============================] - 37s 240ms/step - loss: 0.4816 - accuracy: 0.7755 - val_loss: 0.5758 - val_accuracy: 0.7358\n",
      "Epoch 97/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.4842 - accuracy: 0.7756 - val_loss: 0.5320 - val_accuracy: 0.7403\n",
      "Epoch 98/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.5013 - accuracy: 0.7652 - val_loss: 0.5098 - val_accuracy: 0.7603\n",
      "Epoch 99/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4827 - accuracy: 0.7753 - val_loss: 0.6174 - val_accuracy: 0.6981\n",
      "Epoch 100/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.4718 - accuracy: 0.7809 - val_loss: 0.5680 - val_accuracy: 0.7313\n",
      "Epoch 101/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4926 - accuracy: 0.7710 - val_loss: 0.5391 - val_accuracy: 0.7538\n",
      "Epoch 102/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4786 - accuracy: 0.7737 - val_loss: 0.5210 - val_accuracy: 0.7587\n",
      "Epoch 103/256\n",
      "153/153 [==============================] - 36s 232ms/step - loss: 0.4740 - accuracy: 0.7776 - val_loss: 0.5251 - val_accuracy: 0.7575\n",
      "Epoch 104/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4912 - accuracy: 0.7702 - val_loss: 0.5292 - val_accuracy: 0.7452\n",
      "Epoch 105/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4748 - accuracy: 0.7747 - val_loss: 0.5533 - val_accuracy: 0.7456\n",
      "Epoch 106/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.4742 - accuracy: 0.7783 - val_loss: 0.5032 - val_accuracy: 0.7620\n",
      "Epoch 107/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4658 - accuracy: 0.7836 - val_loss: 0.5648 - val_accuracy: 0.7284\n",
      "Epoch 108/256\n",
      "153/153 [==============================] - 36s 233ms/step - loss: 0.4766 - accuracy: 0.7789 - val_loss: 0.5010 - val_accuracy: 0.7616\n",
      "Epoch 109/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4684 - accuracy: 0.7831 - val_loss: 0.5653 - val_accuracy: 0.7366\n",
      "Epoch 110/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.4762 - accuracy: 0.7819 - val_loss: 0.5197 - val_accuracy: 0.7608\n",
      "Epoch 111/256\n",
      "153/153 [==============================] - 35s 231ms/step - loss: 0.4602 - accuracy: 0.7803 - val_loss: 0.5426 - val_accuracy: 0.7513\n",
      "Epoch 112/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.4671 - accuracy: 0.7794 - val_loss: 0.5214 - val_accuracy: 0.7526\n",
      "Epoch 113/256\n",
      "153/153 [==============================] - 37s 239ms/step - loss: 0.4637 - accuracy: 0.7874 - val_loss: 0.5071 - val_accuracy: 0.7591\n",
      "Epoch 114/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4592 - accuracy: 0.7849 - val_loss: 0.5053 - val_accuracy: 0.7550\n",
      "Epoch 115/256\n",
      "153/153 [==============================] - 37s 239ms/step - loss: 0.4580 - accuracy: 0.7864 - val_loss: 0.5045 - val_accuracy: 0.7657\n",
      "Epoch 116/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.4550 - accuracy: 0.7861 - val_loss: 0.5368 - val_accuracy: 0.7481\n",
      "Epoch 117/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4618 - accuracy: 0.7834 - val_loss: 0.4961 - val_accuracy: 0.7673\n",
      "Epoch 118/256\n",
      "153/153 [==============================] - 37s 240ms/step - loss: 0.4509 - accuracy: 0.7857 - val_loss: 0.5058 - val_accuracy: 0.7558\n",
      "Epoch 119/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4583 - accuracy: 0.7831 - val_loss: 0.5531 - val_accuracy: 0.7390\n",
      "Epoch 120/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4585 - accuracy: 0.7865 - val_loss: 0.6442 - val_accuracy: 0.6989\n",
      "Epoch 121/256\n",
      "153/153 [==============================] - 37s 241ms/step - loss: 0.4734 - accuracy: 0.7769 - val_loss: 0.5355 - val_accuracy: 0.7431\n",
      "Epoch 122/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4486 - accuracy: 0.7939 - val_loss: 0.5254 - val_accuracy: 0.7509\n",
      "Epoch 123/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.4513 - accuracy: 0.7894 - val_loss: 0.5229 - val_accuracy: 0.7587\n",
      "Epoch 124/256\n",
      "153/153 [==============================] - 37s 239ms/step - loss: 0.4452 - accuracy: 0.7927 - val_loss: 0.5653 - val_accuracy: 0.7345\n",
      "Epoch 125/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4722 - accuracy: 0.7836 - val_loss: 0.5355 - val_accuracy: 0.7526\n",
      "Epoch 126/256\n",
      "153/153 [==============================] - 36s 239ms/step - loss: 0.4628 - accuracy: 0.7858 - val_loss: 0.5394 - val_accuracy: 0.7579\n",
      "Epoch 127/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4405 - accuracy: 0.7944 - val_loss: 0.6130 - val_accuracy: 0.7079\n",
      "Epoch 128/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4739 - accuracy: 0.7744 - val_loss: 0.4993 - val_accuracy: 0.7702\n",
      "Epoch 129/256\n",
      "153/153 [==============================] - 37s 241ms/step - loss: 0.4412 - accuracy: 0.7958 - val_loss: 0.5089 - val_accuracy: 0.7640\n",
      "Epoch 130/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4285 - accuracy: 0.7991 - val_loss: 0.5028 - val_accuracy: 0.7653\n",
      "Epoch 131/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4391 - accuracy: 0.7940 - val_loss: 0.5146 - val_accuracy: 0.7550\n",
      "Epoch 132/256\n",
      "153/153 [==============================] - 37s 240ms/step - loss: 0.4435 - accuracy: 0.7947 - val_loss: 0.6719 - val_accuracy: 0.6981\n",
      "Epoch 133/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4584 - accuracy: 0.7880 - val_loss: 0.6302 - val_accuracy: 0.7141\n",
      "Epoch 134/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4426 - accuracy: 0.7960 - val_loss: 0.5121 - val_accuracy: 0.7636\n",
      "Epoch 135/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.4372 - accuracy: 0.7991 - val_loss: 0.6402 - val_accuracy: 0.7112\n",
      "Epoch 136/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4435 - accuracy: 0.7946 - val_loss: 0.5097 - val_accuracy: 0.7616\n",
      "Epoch 137/256\n",
      "153/153 [==============================] - 36s 238ms/step - loss: 0.4361 - accuracy: 0.7996 - val_loss: 0.5299 - val_accuracy: 0.7599\n",
      "Epoch 138/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4292 - accuracy: 0.8015 - val_loss: 0.5045 - val_accuracy: 0.7673\n",
      "Epoch 139/256\n",
      "153/153 [==============================] - 36s 235ms/step - loss: 0.4289 - accuracy: 0.8027 - val_loss: 0.5282 - val_accuracy: 0.7632\n",
      "Epoch 140/256\n",
      "153/153 [==============================] - 37s 239ms/step - loss: 0.4164 - accuracy: 0.8107 - val_loss: 0.6304 - val_accuracy: 0.7255\n",
      "Epoch 141/256\n",
      "153/153 [==============================] - 36s 236ms/step - loss: 0.4250 - accuracy: 0.8034 - val_loss: 0.5087 - val_accuracy: 0.7603\n",
      "Epoch 142/256\n",
      "153/153 [==============================] - 37s 239ms/step - loss: 0.4182 - accuracy: 0.8094 - val_loss: 0.5464 - val_accuracy: 0.7599\n",
      "Epoch 143/256\n",
      "153/153 [==============================] - 37s 241ms/step - loss: 0.4331 - accuracy: 0.8005 - val_loss: 0.5434 - val_accuracy: 0.7558\n",
      "Epoch 144/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4237 - accuracy: 0.8042 - val_loss: 0.5294 - val_accuracy: 0.7628\n",
      "Epoch 145/256\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4243 - accuracy: 0.8069 - val_loss: 0.5117 - val_accuracy: 0.7603\n",
      "Epoch 146/256\n",
      "153/153 [==============================] - 36s 239ms/step - loss: 0.4240 - accuracy: 0.8083 - val_loss: 0.5762 - val_accuracy: 0.7501\n",
      "Epoch 147/256\n",
      "153/153 [==============================] - 36s 234ms/step - loss: 0.4185 - accuracy: 0.8066 - val_loss: 0.5486 - val_accuracy: 0.7448\n",
      "Epoch 148/256\n",
      "153/153 [==============================] - 36s 239ms/step - loss: 0.4262 - accuracy: 0.8055 - val_loss: 0.5286 - val_accuracy: 0.7530\n",
      "Epoch 149/256\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.4056 - accuracy: 0.8180Restoring model weights from the end of the best epoch: 117.\n",
      "153/153 [==============================] - 36s 237ms/step - loss: 0.4056 - accuracy: 0.8180 - val_loss: 0.5978 - val_accuracy: 0.7435\n",
      "Epoch 149: early stopping\n"
     ]
    }
   ],
   "source": [
    "# make key hyperparameters accessible and easy to tune\n",
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "# calculate how many times the model will adjust it's weights per epoch (used for saving checkpoints)\n",
    "SAVE_FREQ = math.ceil(len(X_train) / BATCH_SIZE)\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(labels['label'].unique())\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(LSTM_UNITS, input_shape=(SEQUENCE_LENGTH, num_features)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "cp_cb = ModelCheckpoint(filepath='.checkpoints/cp-{epoch:03d}.ckpt', save_weights_only=True, save_freq=8*SAVE_FREQ)\n",
    "stp_cb = EarlyStopping(monitor='val_loss', patience=32, restore_best_weights=True, min_delta=1e-4, start_from_epoch=32, verbose=1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=256, batch_size=BATCH_SIZE, callbacks=[cp_cb, stp_cb], validation_data=(X_val, y_val))\n",
    "\n",
    "# Save Model\n",
    "model.save(f'models/lstm_{LSTM_UNITS}-seq_{SEQUENCE_LENGTH}-batch_{BATCH_SIZE}.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Last thing to do is verify the models accuracy using the previously split test data. In addition to verifying the model accurary, I used a confusion matrix to inform the prominent prediction errors, which could be used to improve additional model iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 76.77%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model from file instead of training\n",
    "# model = tf.keras.models.load_model(\"model.keras\")\n",
    "\n",
    "# Quick variant used for model training and tuning\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=None)\n",
    "print(\"Model accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "# Build confusion matrix\n",
    "pred = model.predict(X_test, verbose=None)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels[\"label\"].unique())\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=labels[\"label\"].unique())\n",
    "cm_disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Notes\n",
    "\n",
    "The model trained and presented above needs additional work to be able to be sensibly deployed to production. Three major aspects have to be considered:\n",
    "\n",
    "1. The model currently requires specially formatted input, that is different from the `.csv` batch data, that was presented for training. Luckily trainsforming a single batch into 256 reading sequences is fairly simple and the code from above can be reused. Assuming the data of a batch was parsed into the variable `batch` the following snipped can be included as part of the model pipeline.\n",
    "```python\n",
    "batch = batch.sort_values('zeit')\n",
    "for i in range(0, len(batch) - SEQUENCE_LENGTH, SEQUENCE_LENGTH):\n",
    "  sequence = batch.iloc[i:i + SEQUENCE_LENGTH]\n",
    "  sequences.append(sequence[['zeit', 'sensorid', 'messwert']].values)\n",
    "```\n",
    "\n",
    "2. The predictions made by the model (with ~76% accuracy) are for 256 reading sequences instead for a full batch (which I assume to be the goal in deployment). To predict a full batch a simple method like simple majority voting based on the predictions for sequences can be applied to determine the prediction for the batch.\n",
    "\n",
    "3. While the best model has an accuracy of ~76%, it is worthy to point out, that several other models with smaller sequences and fewer LSTM units have achieved almost similar results. In a real world deployment model size and complexity should be considered. I will hand in said best configuration, but at the cost of 3-4% accuracy a significantly smaller model could be used alternatively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
