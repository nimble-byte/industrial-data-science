{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 13:59:14.274621: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-16 13:59:14.293481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-16 13:59:14.293501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-16 13:59:14.294006: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-16 13:59:14.297063: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-16 13:59:14.297646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-16 13:59:14.813401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "First step is to ingest all the data we have available and merge them into a flattened datastructure containing all measurements. Indexes are ignored and rewritten to allow all readings to be added to the DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the labels.csv\n",
    "labels = pd.read_csv('labels.csv', index_col=0)\n",
    "labels = labels.sort_values('id')\n",
    "\n",
    "# grab filenames from the data directory\n",
    "filenames = os.listdir('data')\n",
    "filenames.sort()\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# parse and concatenate all csv files into df\n",
    "for filename in filenames:\n",
    "  if filename.endswith('.csv'):\n",
    "    batch = pd.read_csv(os.path.join('data',filename), index_col=0)\n",
    "    batch['batch'] = int(filename.replace('.csv', ''))\n",
    "    dataframes.append(batch)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# add label column (if it is not already available)\n",
    "if (not 'label' in df.columns):\n",
    "  df = df.merge(labels, left_on=[\"batch\"], right_on=[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_float(inputstr):\n",
    "  hours, minutes, seconds = map(float, inputstr.split(':'))\n",
    "\n",
    "  # return hours * 3600 + minutes * 60 + seconds\n",
    "  # this is sufficient because hours should always be 0\n",
    "  return minutes * 60 + seconds\n",
    "\n",
    "if (not df['sensorid'].dtype == 'int'):\n",
    "  df['sensorid'] = df['sensorid'].astype('int')\n",
    "if (not df['label'].dtype == 'category'):\n",
    "  df['label'] = df['label'].astype('category')\n",
    "if (not df['zeit'].dtype == 'float64'):\n",
    "  df['zeit'] = df['zeit'].apply(time_to_float)\n",
    "\n",
    "# print(df[:10])\n",
    "# print(labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "\n",
    "sequences_df = []\n",
    "sequence_labels_df = []\n",
    "\n",
    "grouped = df.groupby('batch')\n",
    "\n",
    "for batch, readings in grouped:\n",
    "  readings = readings.sort_values('zeit')\n",
    "  for i in range(0, len(readings) - sequence_length + 1, sequence_length):\n",
    "    sequence = readings.iloc[i:i + sequence_length]\n",
    "    sequences_df.append(sequence[['zeit', 'sensorid', 'messwert']].values)\n",
    "    sequence_labels_df.append(sequence['label'].values[0])\n",
    "\n",
    "sequences = np.array(sequences_df)\n",
    "sequence_labels = np.array(sequence_labels_df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, sequence_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 14:00:02.117017: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 37860000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 29s 29ms/step - loss: 0.6007 - accuracy: 0.7154\n",
      "Epoch 2/10\n",
      "986/986 [==============================] - 28s 29ms/step - loss: 0.5861 - accuracy: 0.7217\n",
      "Epoch 3/10\n",
      "986/986 [==============================] - 28s 28ms/step - loss: 0.5852 - accuracy: 0.7230\n",
      "Epoch 4/10\n",
      "986/986 [==============================] - 28s 28ms/step - loss: 0.5875 - accuracy: 0.7209\n",
      "Epoch 5/10\n",
      "986/986 [==============================] - 28s 28ms/step - loss: 0.5851 - accuracy: 0.7202\n",
      "Epoch 6/10\n",
      "986/986 [==============================] - 28s 28ms/step - loss: 0.5804 - accuracy: 0.7249\n",
      "Epoch 7/10\n",
      "986/986 [==============================] - 29s 29ms/step - loss: 0.5865 - accuracy: 0.7206\n",
      "Epoch 8/10\n",
      "986/986 [==============================] - 29s 29ms/step - loss: 0.5895 - accuracy: 0.7215\n",
      "Epoch 9/10\n",
      "986/986 [==============================] - 28s 28ms/step - loss: 0.5866 - accuracy: 0.7224\n",
      "Epoch 10/10\n",
      "986/986 [==============================] - 28s 29ms/step - loss: 0.5770 - accuracy: 0.7264\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "CHECKPOINT_PATH = '.checkpoints/cp-{epoch:04d}.ckpt'\n",
    "CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_PATH)\n",
    "N_BATCHES = math.ceil(len(X_train) / BATCH_SIZE)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_PATH, save_weights_only=True, save_freq=5*N_BATCHES, verbose=1)\n",
    "\n",
    "def create_model():\n",
    "  clf = Sequential()\n",
    "  clf.add(LSTM(100, input_shape=(sequence_length, 3)))\n",
    "  clf.add(Dense(3, activation='softmax'))\n",
    "  clf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return clf\n",
    "\n",
    "if (os.path.exists('classifier.keras')):\n",
    "  clf = tf.keras.models.load_model('classifier.keras')\n",
    "else:\n",
    "  # Build Model\n",
    "  clf = create_model()\n",
    "\n",
    "# Training\n",
    "clf.fit(X_train, y_train, epochs=100, batch_size=BATCH_SIZE)\n",
    "# Save Model\n",
    "clf.save('classifier.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional save (different names)\n",
    "# clf.save('classifier_softmax.keras')\n",
    "# clf.save('classifier_relu.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6006531119346619,\n",
       " 0.5860811471939087,\n",
       " 0.5851655602455139,\n",
       " 0.5875413417816162,\n",
       " 0.585140585899353,\n",
       " 0.5804042816162109,\n",
       " 0.5864750146865845,\n",
       " 0.5895113945007324,\n",
       " 0.5865926742553711,\n",
       " 0.5770363211631775]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TBD\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
