{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Since this is a handin exercise I will shortly outline the setup required for this notebook to run (assuming I can only hand in the `.ipynb` file). The notebook pulls from the dependencies in the first code block. To install all relevant libraries run (assuming you have `jupyter` installed):\n",
    "\n",
    "```shell\n",
    "python -m pip install pandas tensorflow sklearn\n",
    "```\n",
    "\n",
    "Additionally it required the data and labels to be available as follows:\n",
    "1. the batch data is located in a `data` folder next to the notebook\n",
    "2. the label information is stored as `labels.csv` next to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "First the data needs to be loaded (first codeblock) and then go through some minor transformations before building the training, test and validation datasets. For this purpose all data is loaded into a flat dataframe that includes the batch and corresponding label in addition to the actual features `zeit`, `sensorid` und `messwert`. In preparation for the model training the `zeit` is transformed to float values and the data types of `sensorid` and `label` are adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the labels.csv\n",
    "labels = pd.read_csv('labels.csv', index_col=0)\n",
    "labels = labels.sort_values('id')\n",
    "\n",
    "# grab filenames from the data directory\n",
    "filenames = os.listdir('data')\n",
    "filenames.sort()\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# parse and concatenate all csv files into df\n",
    "for filename in filenames:\n",
    "  if filename.endswith('.csv'):\n",
    "    batch = pd.read_csv(os.path.join('data',filename), index_col=0)\n",
    "    batch['batch'] = int(filename.replace('.csv', ''))\n",
    "    dataframes.append(batch)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# clean up original dataframes\n",
    "del dataframes\n",
    "\n",
    "# add label column (if it is not already available)\n",
    "if (not 'label' in df.columns):\n",
    "  df = df.merge(labels, left_on=[\"batch\"], right_on=[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_float(inputstr):\n",
    "  hours, minutes, seconds = map(float, inputstr.split(':'))\n",
    "\n",
    "  # return hours * 3600 + minutes * 60 + seconds\n",
    "  # this is sufficient because hours should always be 0\n",
    "  return minutes * 60 + seconds\n",
    "\n",
    "if (not df['sensorid'].dtype == 'int'):\n",
    "  df['sensorid'] = df['sensorid'].astype('int')\n",
    "if (not df['label'].dtype == 'category'):\n",
    "  df['label'] = df['label'].astype('category')\n",
    "if (not df['zeit'].dtype == 'float64'):\n",
    "  df['zeit'] = df['zeit'].apply(time_to_float)\n",
    "\n",
    "# print(df[:10])\n",
    "# print(labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Data\n",
    "\n",
    "This step was only introduced once I decided to use a recurrent neural network (specifically an LSTM). The flat dataframe created before has to be broken down into small sequences that will be fed to the model. As to not created sequences with mixed labels, the data was grouped by `batch`. Additionally it is important that each batch is sorted by `zeit` since order of readings in sequences is relevant. After splitting each batch into equally sized sequences (dropping any additional readings at the end that were not able to make a full sequence), the data is split into training, test and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 256\n",
    "\n",
    "sequences = []\n",
    "sequence_labels = []\n",
    "\n",
    "# build sequences based on chosen sequence length\n",
    "for batch, readings in df.groupby('batch'):\n",
    "  readings = readings.sort_values('zeit')\n",
    "  for i in range(0, len(readings) - SEQUENCE_LENGTH, SEQUENCE_LENGTH):\n",
    "    sequence = readings.iloc[i:i + SEQUENCE_LENGTH]\n",
    "    sequences.append(sequence[['zeit', 'sensorid', 'messwert']].values)\n",
    "    sequence_labels.append(sequence['label'].values[0])\n",
    "\n",
    "# transform to numpy arrays for tensorflow\n",
    "sequences = np.array(sequences)\n",
    "sequence_labels = np.array(sequence_labels)\n",
    "\n",
    "# split into train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, sequence_labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling & Training\n",
    "\n",
    "The initial plan was to train a simple NN, that is fed `zeit`, `sensorid` and `messwert` as features to predict the label, but it became apparent quickly, that this approach would not lead to the reuqired accurracy. After some additional research, I settled on an LSTM which is able to detect and learn patterns in the sequences. Batch size and the number of LSTM units was determined using a grid search and running it over night to determine the best `SEQUENCE_LENGTH`, `LSTM_UNITS` and `BATCH_SIZE`. The grid search revealed, that among the chosen options the largest options for `SEQUENCE_LENGTH` and `LSTM_UNITS` performed the best with an accuracy of about 75%.\n",
    "\n",
    "The model itself is implemented as Sequential Model using Tensorflow keras. It consists of 2 layers:\n",
    "\n",
    "- the aforementioned LSTM layer, handling the input and pattern detection\n",
    "- a densely connected layer using softmax activations to produce the predictions\n",
    "\n",
    "The model was then trained using the adam optimiser and set up for early training termination, based on the validation loss each round to prevent overfitting. Each 8 epochs the model weights were saved, as a safety measure should model training crash at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make key hyperparameters accessible and easy to tune\n",
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "# calculate how many times the model will adjust it's weights per epoch (used for saving checkpoints)\n",
    "SAVE_FREQ = math.ceil(len(X_train) / BATCH_SIZE)\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(labels['label'].unique())\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(LSTM_UNITS, input_shape=(SEQUENCE_LENGTH, num_features)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "cp_cb = ModelCheckpoint(filepath='.checkpoints/cp-{epoch:03d}.ckpt', save_weights_only=True, save_freq=8*SAVE_FREQ)\n",
    "stp_cb = EarlyStopping(monitor='val_loss', patience=16, restore_best_weights=True, min_delta=1e-4, start_from_epoch=32, verbose=1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=256, batch_size=BATCH_SIZE, callbacks=[cp_cb, stp_cb], validation_data=(X_val, y_val))\n",
    "\n",
    "# Save Model\n",
    "model.save(f'models/lstm_{LSTM_UNITS}-seq_{SEQUENCE_LENGTH}-batch_{BATCH_SIZE}.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Last thing to do is verify the models accuracy using the previously split test data. In addition to verifying the model accurary, I used a confusion matrix to inform the prominent prediction errors, which could be used to improve additional model iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"models/BEST.seq_256-lstm_256-batch_64.keras\")\n",
    "\n",
    "# Quick variant used for model training and tuning\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=None)\n",
    "print(\"Model accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "# Build confusion matrix\n",
    "pred = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels[\"label\"].unique())\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=labels[\"label\"].unique())\n",
    "cm_disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Notes\n",
    "\n",
    "The model trained and presented above needs additional work to be able to be sensibly deployed to production. Two major aspects have to be considered:\n",
    "\n",
    "1. The model currently requires specially formatted input, that is different from the `.csv` batch data, that was presented for training. Luckily trainsforming a single batch into 256 reading sequences is fairly simple and the code from above can be reused. Assuming the data of a batch was parsed into the variable `batch` the following snipped can be included as part of the model pipeline.\n",
    "```python\n",
    "batch = batch.sort_values('zeit')\n",
    "for i in range(0, len(batch) - SEQUENCE_LENGTH, SEQUENCE_LENGTH):\n",
    "  sequence = batch.iloc[i:i + SEQUENCE_LENGTH]\n",
    "  sequences.append(sequence[['zeit', 'sensorid', 'messwert']].values)\n",
    "```\n",
    "\n",
    "2. The predictions made by the model (with ~75% accuracy) are for 256 reading sequences and for a full batch (which I assume to be the goal in deployment). To predict a full batch a simple method like simple majority voting based on the predictions for sequences can be applied to determine the prediction for the batch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
